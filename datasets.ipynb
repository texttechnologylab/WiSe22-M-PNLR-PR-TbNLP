{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer-based Natural Language Processing\n",
    "## Introduction to PyTorch & the ðŸ¤— Framework\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/texttechnologylab/WiSe22-M-PNLR-PR-TbNLP/blob/master/datasets.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aqcuiring Some Data\n",
    "\n",
    "- Use the code below to accquire some sentence-segmented data.\n",
    "    - Note: You may use also any other corpus available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜dataâ€™: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 24.3M  100 24.3M    0     0  34.2M      0 --:--:-- --:--:-- --:--:-- 34.2M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 25.9M  100 25.9M    0     0  43.9M      0 --:--:-- --:--:-- --:--:-- 43.9M\n"
     ]
    }
   ],
   "source": [
    "# Dowload a small dataset of sentences from the English Wikipedia from the \"Wortschatz\" project of the University Leipzig\n",
    "# - D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.\n",
    "#   In: Proceedings of the 8th International Language Resources and Evaluation (LREC'12), 2012\n",
    "!mkdir data\n",
    "\n",
    "!curl http://pcai056.informatik.uni-leipzig.de/downloads/corpora/eng_wikipedia_2016_100K.tar.gz -o data/eng_wikipedia_2016_100K.tar.gz\n",
    "!curl http://pcai056.informatik.uni-leipzig.de/downloads/corpora/eng_news_2016_100K.tar.gz -o data/eng_news_2016_100K.tar.gz\n",
    "!tar -xf data/eng_wikipedia_2016_100K.tar.gz -C data/\n",
    "!tar -xf data/eng_news_2016_100K.tar.gz -C data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing necessary packages (i.e. if on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch datasets tokenizers transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with `ðŸ¤— Datasets`\n",
    "\n",
    "1. Familiarize yourself with the `ðŸ¤— Datasets` package and it's API.\n",
    "2. [Load](https://huggingface.co/docs/datasets/loading#local-and-remote-files) the plain text corpus that was downloaded using the code above.\n",
    "3. [(Pre-)Process](https://huggingface.co/docs/datasets/process#map) the data:\n",
    "    - Remove the line-number preceding each sentence.\n",
    "    - Split the sentences into words/tokens.\n",
    "\n",
    "\n",
    "#### Resources\n",
    "\n",
    "- [`ðŸ¤— datasets` Documentation](https://huggingface.co/docs/datasets/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia_file = \"data/eng_wikipedia_2016_100K/eng_wikipedia_2016_100K-sentences.txt\"\n",
    "news_file = \"data/eng_wikipedia_2016_100K/eng_wikipedia_2016_100K-sentences.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0fa1bc84c0851a77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/mastoeck/.cache/huggingface/datasets/text/default-0fa1bc84c0851a77/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mastoeck/anaconda3/envs/torch310/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f03378db38834cebb85916e8cf137442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c5f9eb874194cd7956fd3f8ff71c638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/mastoeck/.cache/huggingface/datasets/text/default-0fa1bc84c0851a77/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad. Subsequent calls will reuse this data.\n",
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 200000\n",
      "})\n",
      "{'text': '1\\t0.41% of the population were Hispanic or Latino of any race.'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f32f5653a7154c62a2e8fdf422769b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '0.41% of the population were Hispanic or Latino of any race.'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "corpus = load_dataset('text', data_files={'train': [wikipedia_file, news_file]}, split=\"train\")\n",
    "print(corpus)\n",
    "print(corpus[0])\n",
    "\n",
    "\n",
    "def remove_line_index(examples):\n",
    "    for i, sample in enumerate(examples['text']):\n",
    "        examples['text'][i] = ' '.join(sample.strip().split()[1:])\n",
    "    return examples\n",
    "\n",
    "\n",
    "corpus = corpus.map(remove_line_index, batched=True)\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with `ðŸ¤— tokenizers`\n",
    "\n",
    "1. Implement a tokenization approach using the `ðŸ¤— tokenizers` library.\n",
    "    - There are [multiple different models](https://huggingface.co/docs/tokenizers/python/latest/components.html#models) of tokenizers available. Which one do you choose for the task at hand?\n",
    "2. Tokenize your dataset using the new tokenizer and rerun your experiment from above.\n",
    "3. Evaluate the results and compare them with the results from above.\n",
    "\n",
    "#### Resources\n",
    "\n",
    "- [`ðŸ¤— Tokenizers` Documentation](https://huggingface.co/docs/tokenizers/python/latest/)\n",
    "- [`ðŸ¤— Transformers` \"Use tokenizers from ðŸ¤— Tokenizers\"](https://huggingface.co/docs/transformers/main/en/fast_tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964478a9732f467a91997fadfa906bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Tokenizer from Iterator:   0%|          | 0/200 [00:00<?, ? batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import trange\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers\n",
    "\n",
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.NFKC(), normalizers.Lowercase()])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([pre_tokenizers.UnicodeScripts(), pre_tokenizers.Whitespace()])\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(\n",
    "    vocab_size=32000,\n",
    "    min_frequency=50,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\"],\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "\n",
    "def _batch_iterator(batch_size=1000):\n",
    "    for i in trange(0, len(corpus), batch_size, desc=\"Training Tokenizer from Iterator\", unit=\" batches\"):\n",
    "        yield corpus[i: i + batch_size][\"text\"]\n",
    "\n",
    "\n",
    "tokenizer.train_from_iterator(_batch_iterator(), trainer=trainer)\n",
    "\n",
    "unk_id = tokenizer.token_to_id(tokenizer.model.unk_token)\n",
    "pad_id = tokenizer.token_to_id('[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 10, 31, 157, 11, 2640, 4, 15, 10, 55, 1, 469, 162, 1527, 91]\n",
      "[0, 55, 2640, 1, 1527]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"This is an   example-sentence, that is some 123456 words long!Â²\").ids)\n",
    "print(tokenizer.encode(\"[PAD] Some sentence alkgjÃ¶eo!\").ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "raises-exception": true,
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Encoding(num_tokens=15, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]), Encoding(num_tokens=5, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m batch \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m      2\u001b[0m     [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an   example-sentence, that is some 123456 words long!Â²\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD] Some sentence alkgjÃ¶eo!\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mids\u001b[49m)  \u001b[38;5;66;03m# will raise AttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ids'"
     ]
    }
   ],
   "source": [
    "batch = tokenizer.encode_batch(\n",
    "    [\"This is an   example-sentence, that is some 123456 words long!Â²\", \"[PAD] Some sentence alkgjÃ¶eo!\"])\n",
    "print(batch)\n",
    "\n",
    "print(batch.ids)  # will raise AttributeError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert your tokenizer to a `ðŸ¤— Transformer` tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply your tokenizer to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01245f46c00042648d0dd931efe71ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode(batch: dict):\n",
    "    return fast_tokenizer(\n",
    "        batch['text'],\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "\n",
    "dataset = corpus.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['0.41% of the population were Hispanic or Latino of any race.', \"'06 DTAs appear to have made essentially simultaneous and duplicative amendments to the Code and its notes.\", '; 1011 Configuration Write : This operates analogously to a configuration read.', \"10 May 2007 Former IRA members Anthony McIntyre and Richard O'Rawe have claimed Adams was a key figure in the IRA.\", '; 1111 Memory Write and Invalidate : This command is identical to a generic memory write, but comes with the guarantee that one or more whole cache lines will be written, with all byte selects enabled.'], 'input_ids': [[270, 3, 2798, 139, 5, 2, 336, 33, 5504, 28, 1, 5, 110, 1147, 3], [17, 1, 1, 843, 7, 37, 100, 2502, 6760, 6, 1, 1, 7, 2, 598, 6, 47, 1330, 3], [58, 1, 2664, 1246, 42, 29, 3606, 1, 7, 9, 2664, 907, 3], [204, 68, 521, 588, 4958, 227, 4775, 1, 6, 1450, 819, 17, 1, 37, 1285, 3901, 12, 9, 600, 1373, 8, 2, 4958, 3], [58, 1, 792, 1246, 6, 1, 42, 29, 1146, 10, 2695, 7, 9, 7433, 792, 1246, 4, 41, 1342, 20, 2, 5860, 15, 44, 28, 49, 822, 6207, 632, 95, 26, 389, 4, 20, 51, 5382, 1, 4709, 3]]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [tensor([ 270,    3, 2798,  139,    5,    2,  336,   33, 5504,   28,    1,    5,\n",
      "         110, 1147,    3]), tensor([  17,    1,    1,  843,    7,   37,  100, 2502, 6760,    6,    1,    1,\n",
      "           7,    2,  598,    6,   47, 1330,    3]), tensor([  58,    1, 2664, 1246,   42,   29, 3606,    1,    7,    9, 2664,  907,\n",
      "           3]), tensor([ 204,   68,  521,  588, 4958,  227, 4775,    1,    6, 1450,  819,   17,\n",
      "           1,   37, 1285, 3901,   12,    9,  600, 1373,    8,    2, 4958,    3]), tensor([  58,    1,  792, 1246,    6,    1,   42,   29, 1146,   10, 2695,    7,\n",
      "           9, 7433,  792, 1246,    4,   41, 1342,   20,    2, 5860,   15,   44,\n",
      "          28,   49,  822, 6207,  632,   95,   26,  389,    4,   20,   51, 5382,\n",
      "           1, 4709,    3])]}\n"
     ]
    }
   ],
   "source": [
    "pt_dataset = dataset.with_format('torch', columns=['input_ids'])\n",
    "print(pt_dataset[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 270,    3, 2798,  139,    5,    2,  336,   33, 5504,   28,    1,    5,\n",
      "          110, 1147,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [  17,    1,    1,  843,    7,   37,  100, 2502, 6760,    6,    1,    1,\n",
      "            7,    2,  598,    6,   47, 1330,    3,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [  58,    1, 2664, 1246,   42,   29, 3606,    1,    7,    9, 2664,  907,\n",
      "            3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 204,   68,  521,  588, 4958,  227, 4775,    1,    6, 1450,  819,   17,\n",
      "            1,   37, 1285, 3901,   12,    9,  600, 1373,    8,    2, 4958,    3,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [  58,    1,  792, 1246,    6,    1,   42,   29, 1146,   10, 2695,    7,\n",
      "            9, 7433,  792, 1246,    4,   41, 1342,   20,    2, 5860,   15,   44,\n",
      "           28,   49,  822, 6207,  632,   95,   26,  389,    4,   20,   51, 5382,\n",
      "            1, 4709,    3]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "pt_dataset.set_transform(\n",
    "    lambda batch: batch | {'input_ids': pad_sequence(map(torch.tensor, batch['input_ids']), batch_first=True, padding_value=pad_id)},\n",
    "    columns=['input_ids']\n",
    ")\n",
    "print(pt_dataset[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 270,    3, 2798,  139,    5,    2,  336,   33, 5504,   28,    1,    5,\n",
      "          110, 1147,    3,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [  17,    1,    1,  843,    7,   37,  100, 2502, 6760,    6,    1,    1,\n",
      "            7,    2,  598,    6,   47, 1330,    3,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [  58,    1, 2664, 1246,   42,   29, 3606,    1,    7,    9, 2664,  907,\n",
      "            3,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 204,   68,  521,  588, 4958,  227, 4775,    1,    6, 1450,  819,   17,\n",
      "            1,   37, 1285, 3901,   12,    9,  600, 1373,    8,    2, 4958,    3,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [  58,    1,  792, 1246,    6,    1,   42,   29, 1146,   10, 2695,    7,\n",
      "            9, 7433,  792, 1246,    4,   41, 1342,   20,    2, 5860,   15,   44,\n",
      "           28,   49,  822, 6207,  632,   95,   26,  389,    4,   20,   51, 5382,\n",
      "            1, 4709,    3]])}\n"
     ]
    }
   ],
   "source": [
    "# Recommended way?\n",
    "# https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.with_transform\n",
    "def encode_pt(batch: dict):\n",
    "    return fast_tokenizer(\n",
    "        batch['text'],\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "jit_dataset = corpus.with_transform(encode_pt)\n",
    "print(jit_dataset[0:5])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "interpreter": {
   "hash": "1b267c91f0e969427353ee359f0d79efaf7083d633af26cd8d79a96b1a90613d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
