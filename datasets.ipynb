{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer-based Natural Language Processing\n",
    "## Introduction to PyTorch & the ðŸ¤— Framework\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/texttechnologylab/WiSe22-M-PNLR-PR-TbNLP/blob/master/datasets.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aqcuiring Some Data\n",
    "\n",
    "- Use the code below to accquire some sentence-segmented data.\n",
    "    - Note: You may use also any other corpus available to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowload a small dataset of sentences from the English Wikipedia from the \"Wortschatz\" project of the University Leipzig\n",
    "# - D. Goldhahn, T. Eckart & U. Quasthoff: Building Large Monolingual Dictionaries at the Leipzig Corpora Collection: From 100 to 200 Languages.\n",
    "#   In: Proceedings of the 8th International Language Resources and Evaluation (LREC'12), 2012\n",
    "!mkdir data\n",
    "!curl http://pcai056.informatik.uni-leipzig.de/downloads/corpora/eng_wikipedia_2016_10K.tar.gz -o data/eng_wikipedia_2016_10K.tar.gz\n",
    "!tar -xf data/eng_wikipedia_2016_10K.tar.gz -C data/\n",
    "\n",
    "plain_text_file = \"data/eng_wikipedia_2016_10K/eng_wikipedia_2016_10K-sentences.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing necessary packages (i.e. if on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch datasets tokenizers transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with `ðŸ¤— Datasets`\n",
    "\n",
    "1. Familiarize yourself with the `ðŸ¤— Datasets` package and it's API.\n",
    "2. [Load](https://huggingface.co/docs/datasets/loading#local-and-remote-files) the plain text corpus that was downloaded using the code above.\n",
    "3. [(Pre-)Process](https://huggingface.co/docs/datasets/process#map) the data:\n",
    "    - Remove the line-number preceding each sentence.\n",
    "    - Split the sentences into words/tokens.\n",
    "\n",
    "\n",
    "#### Resources\n",
    "\n",
    "- [`ðŸ¤— datasets` Documentation](https://huggingface.co/docs/datasets/index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ...  # TODO\n",
    "\n",
    "corpus = ...  # TODO: Load the plain text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with `ðŸ¤— tokenizers`\n",
    "\n",
    "1. Implement a tokenization approach using the `ðŸ¤— tokenizers` library.\n",
    "    - There are [multiple different models](https://huggingface.co/docs/tokenizers/python/latest/components.html#models) of tokenizers available. Which one do you choose for the task at hand?\n",
    "2. Tokenize your dataset using the new tokenizer and rerun your experiment from above.\n",
    "3. Evaluate the results and compare them with the results from above.\n",
    "\n",
    "#### Resources\n",
    "\n",
    "- [`ðŸ¤— Tokenizers` Documentation](https://huggingface.co/docs/tokenizers/python/latest/)\n",
    "- [`ðŸ¤— Transformers` \"Use tokenizers from ðŸ¤— Tokenizers\"](https://huggingface.co/docs/transformers/main/en/fast_tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "# from tokenizers.X import Y\n",
    "\n",
    "tokenizer = Tokenizer(...)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convert your tokenizer to a `ðŸ¤— Transformer` tokenizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Apply your tokenizer to the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_tokenizers(*args, **kwargs):  # TODO\n",
    "    pass\n",
    "\n",
    "dataset = ...  # TODO: Process the corpus"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b267c91f0e969427353ee359f0d79efaf7083d633af26cd8d79a96b1a90613d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 ('py310-pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
