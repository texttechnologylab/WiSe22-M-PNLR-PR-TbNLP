{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer-based Natural Language Processing\n",
    "## Introduction to ðŸ¤— Transformers\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/texttechnologylab/WiSe22-M-PNLR-PR-TbNLP/blob/master/transformers.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing necessary packages (i.e. if on Colab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "% pip install torch datasets tokenizers transformers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Premise\n",
    "\n",
    "This notebook will guide you through the process of finetuning a transformer model using the ðŸ¤— Transformers library.\n",
    "\n",
    "First, we need to select a task and suitable dataset. Here, we will use the [Textual Entailment](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) task as an example. A suitable dataset can be found in the [SuperGLUE repository](https://www.tensorflow.org/datasets/catalog/super_glue#super_gluerte). While this is a TensorFlow dataset, we are lucky as it is mirrored on the ðŸ¤— Dataset hub.\n",
    "\n",
    "Thus, we can load the Recognizing Textual Entailment subset of the SuperGLUE dataset as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 2490\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 277\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'idx', 'label'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "rte_dataset = load_dataset(\"super_glue\", \"rte\")\n",
    "print(rte_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see above, the dataset is already split into train, development and test splits.\n",
    "Each row contains four, but we only need to focus the premise, hypothesis and the label.\n",
    "\n",
    "The textual entailment task requires us to recognize, given two text fragments, whether the meaning of one text is entailed (*can be inferred*) from the other text.\n",
    "\n",
    "In this example, we will use a BERT-family model. With BERT, we formulate the entailment task as a simple classification task by concatenating the premise and hypothesis and training our classifier on the first token (the `[CLS]` token) of the input string:\n",
    "\n",
    "```\n",
    "\"[CLS] This is the premise, i.e. a text that means something. [SEP] This is the hypothesis, i.e. what we may be able to infer [SEP]\"\n",
    "```\n",
    "\n",
    "But let's first take a look at the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': ['No Weapons of Mass Destruction Found in Iraq Yet.', 'A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.'], 'hypothesis': ['Weapons of Mass Destruction Found in Iraq.', 'Pope Benedict XVI is the new leader of the Roman Catholic Church.'], 'idx': [0, 1], 'label': [1, 0]}\n",
      "{'premise': ['Dana Reeve, the widow of the actor Christopher Reeve, has died of lung cancer at age 44, according to the Christopher Reeve Foundation.', 'Yet, we now are discovering that antibiotics are losing their effectiveness against illness. Disease-causing bacteria are mutating faster than we can come up with new antibiotics to fight the new variations.'], 'hypothesis': ['Christopher Reeve had an accident.', 'Bacteria is winning the war against antibiotics.'], 'idx': [0, 1], 'label': [1, 0]}\n",
      "{'premise': [\"Mangla was summoned after Madhumita's sister Nidhi Shukla, who was the first witness in the case.\", \"Authorities in Brazil say that more than 200 people are being held hostage in a prison in the country's remote, Amazonian-jungle state of Rondonia.\"], 'hypothesis': ['Shukla is related to Mangla.', 'Authorities in Brazil hold 200 people as hostage.'], 'idx': [0, 1], 'label': [-1, -1]}\n"
     ]
    }
   ],
   "source": [
    "print(rte_dataset['train'][:2])\n",
    "print(rte_dataset['validation'][:2])\n",
    "print(rte_dataset['test'][:2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we see above, the `test` split contains **unlabeled** samples, be we can ignore that for now.\n",
    "\n",
    "Let's construct the sentences as we outlined above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [1, 0], 'input': ['No Weapons of Mass Destruction Found in Iraq Yet. [SEP] Weapons of Mass Destruction Found in Iraq.', 'A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI. [SEP] Pope Benedict XVI is the new leader of the Roman Catholic Church.']}\n"
     ]
    }
   ],
   "source": [
    "prepared_dataset = rte_dataset.map(\n",
    "    lambda sample: {'input': f\"{sample['premise']} [SEP] {sample['hypothesis']}\"},\n",
    "    remove_columns=['premise', 'hypothesis', 'idx']\n",
    ")\n",
    "print(prepared_dataset['train'][:2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Pre-Trained Models\n",
    "\n",
    "Now we need to load a pre-trained BERT model. You should use a subclass of [AutoModel](https://huggingface.co/docs/transformers/main/en/autoclass_tutorial).\n",
    "\n",
    "#### Load and instantiate a model for the textual entailment task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer  #, AutoModelFor?\n",
    "\n",
    "config = ...  # TODO\n",
    "tokenizer = ...  # TODO\n",
    "model = ...  # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we could use the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class for easy training. You can follow the tutorial from [the official documentation](https://huggingface.co/docs/transformers/quicktour#trainer-a-pytorch-optimized-training-loop).\n",
    "\n",
    "#### Write the training procedure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(...)\n",
    "\n",
    "# TODO\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# TODO: evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom Training\n",
    "While using the trainer class is very convenient, if you have to run custom procedures during training, a regular training loop can be more accessible.\n",
    "\n",
    "We can re-use code from the datasets notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode_pt(batch: dict):\n",
    "    return tokenizer(\n",
    "        batch['input'],\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "pt_dataset = prepared_dataset.map(encode_pt)\n",
    "print(pt_dataset['train'][:2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, in a manual training loop, we will want to make use of PyTorch's DataLoaders, which require some extra care to collate batches with samples of different lengths.\n",
    "\n",
    "#### Implement `custom_collate`:\n",
    "- Pad and stack the `input_ids` in a tensor.\n",
    "- Stack the labels in a tensor of type `long`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def custom_collate(batch: list[dict]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    input_ids = ...\n",
    "    label = ...\n",
    "    return input_ids, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Write the training and evaluation loops"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import *  # TODO\n",
    "from torch.nn import *  # TODO\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "criterion = ...  # TODO\n",
    "optimizer = ...  # TODO\n",
    "num_epochs = ...  # TODO\n",
    "batch_size = ...  # TODO\n",
    "\n",
    "train_dataloader = DataLoader(pt_dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "dev_dataloader = DataLoader(pt_dataset['validation'], batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "model.to(device)\n",
    "for epoch in trange(num_epochs, position=0):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, position=1, leave=False):\n",
    "        ...  # TODO\n",
    "\n",
    "    model.eval()\n",
    "    for batch in tqdm(dev_dataloader, position=1, leave=False):\n",
    "        ...  # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
