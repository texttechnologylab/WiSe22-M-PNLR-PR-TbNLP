{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transformer-based Natural Language Processing\n",
    "## Introduction to ðŸ¤— Transformers\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/texttechnologylab/WiSe22-M-PNLR-PR-TbNLP/blob/master/transformers.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing necessary packages (i.e. if on Colab)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "% pip install torch datasets tokenizers transformers"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Premise\n",
    "\n",
    "This notebook will guide you through the process of finetuning a transformer model using the ðŸ¤— Transformers library.\n",
    "\n",
    "First, we need to select a task and suitable dataset. Here, we will use the [Textual Entailment](https://nlp.stanford.edu/projects/snli/) task as an example. A suitable dataset can be found in the [`snli` repository on the ðŸ¤— Dataset hub](https://huggingface.co/datasets/snli).\n",
    "\n",
    "Thus, we can load the Recognizing Textual Entailment subset of the SuperGLUE dataset as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 550152\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from datasets import disable_caching\n",
    "disable_caching()\n",
    "\n",
    "snli_dataset = load_dataset(\"snli\")\n",
    "print(snli_dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we can see above, the dataset is already split into train, development and test splits.\n",
    "Each row contains four, but we only need to focus the premise, hypothesis and the label.\n",
    "\n",
    "The textual entailment task requires us to recognize, given two text fragments, whether the meaning of one text is entailed (*can be inferred*) from the other text.\n",
    "\n",
    "In this example, we will use a BERT-family model. With BERT, we formulate the entailment task as a simple classification task by concatenating the premise and hypothesis and training our classifier on the first token (the `[CLS]` token) of the input string:\n",
    "\n",
    "```\n",
    "\"[CLS] This is the premise, i.e. a text that means something. [SEP] This is the hypothesis, i.e. what we may be able to infer [SEP]\"\n",
    "```\n",
    "\n",
    "But let's first take a look at the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': ['A person on a horse jumps over a broken down airplane.', 'A person on a horse jumps over a broken down airplane.'], 'hypothesis': ['A person is training his horse for a competition.', 'A person is at a diner, ordering an omelette.'], 'label': [1, 2]}\n",
      "{'premise': ['Two women are embracing while holding to go packages.', 'Two women are embracing while holding to go packages.'], 'hypothesis': ['The sisters are hugging goodbye while holding to go packages after just eating lunch.', 'Two woman are holding packages.'], 'label': [1, 0]}\n",
      "{'premise': ['This church choir sings to the masses as they sing joyous songs from the book at a church.', 'This church choir sings to the masses as they sing joyous songs from the book at a church.'], 'hypothesis': ['The church has cracks in the ceiling.', 'The church is filled with song.'], 'label': [1, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(snli_dataset['train'][:2])\n",
    "print(snli_dataset['validation'][:2])\n",
    "print(snli_dataset['test'][:2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we see above, the `test` split contains **unlabeled** samples, be we can ignore that for now.\n",
    "\n",
    "Let's construct the sentences as we outlined above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [1, 2, 0], 'input': ['A person on a horse jumps over a broken down airplane. [SEP] A person is training his horse for a competition.', 'A person on a horse jumps over a broken down airplane. [SEP] A person is at a diner, ordering an omelette.', 'A person on a horse jumps over a broken down airplane. [SEP] A person is outdoors, on a horse.']}\n"
     ]
    }
   ],
   "source": [
    "prepared_dataset = snli_dataset.filter(lambda sample: sample['label'] >= 0)\n",
    "prepared_dataset = prepared_dataset.map(\n",
    "    lambda sample: {'input': f\"{sample['premise']} [SEP] {sample['hypothesis']}\"},\n",
    "    remove_columns=['premise', 'hypothesis']\n",
    ")\n",
    "print(prepared_dataset['train'][:3])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loading Pre-Trained Models\n",
    "\n",
    "Now we need to load a pre-trained BERT model. You should use a subclass of [AutoModel](https://huggingface.co/docs/transformers/main/en/autoclass_tutorial).\n",
    "\n",
    "#### Load and instantiate a model for the textual entailment task"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer  #, AutoModelFor?\n",
    "\n",
    "config = ...  # TODO\n",
    "tokenizer = ...  # TODO\n",
    "model = ...  # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we could use the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class for easy training. You can follow the tutorial from [the official documentation](https://huggingface.co/docs/transformers/quicktour#trainer-a-pytorch-optimized-training-loop).\n",
    "\n",
    "#### Write the training procedure"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(...)\n",
    "\n",
    "# TODO\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# TODO: evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom Training\n",
    "While using the trainer class is very convenient, if you have to run custom procedures during training, a regular training loop can be more accessible.\n",
    "\n",
    "We can re-use code from the datasets notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def encode_pt(batch: dict):\n",
    "    return tokenizer(\n",
    "        batch['input'],\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "\n",
    "pt_dataset = prepared_dataset.map(encode_pt)\n",
    "print(pt_dataset['train'][:2])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, in a manual training loop, we will want to make use of PyTorch's DataLoaders, which require some extra care to collate batches with samples of different lengths.\n",
    "\n",
    "#### Implement `custom_collate`:\n",
    "- Pad and stack the `input_ids` in a tensor.\n",
    "- Stack the labels in a tensor of type `long`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def custom_collate(batch: list[dict]) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    input_ids = ...\n",
    "    label = ...\n",
    "    return input_ids, label"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Write the training and evaluation loops"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import *  # TODO\n",
    "from torch.nn import *  # TODO\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "criterion = ...  # TODO\n",
    "optimizer = ...  # TODO\n",
    "num_epochs = ...  # TODO\n",
    "batch_size = ...  # TODO\n",
    "\n",
    "train_dataloader = DataLoader(pt_dataset['train'], batch_size=batch_size, shuffle=True, collate_fn=custom_collate)\n",
    "dev_dataloader = DataLoader(pt_dataset['validation'], batch_size=batch_size, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "model.to(device)\n",
    "for epoch in trange(num_epochs, position=0):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dataloader, position=1, leave=False):\n",
    "        ...  # TODO\n",
    "\n",
    "    model.eval()\n",
    "    for batch in tqdm(dev_dataloader, position=1, leave=False):\n",
    "        ...  # TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
