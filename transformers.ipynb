{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer-based Natural Language Processing\n",
    "## Introduction to ðŸ¤— Transformers\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/texttechnologylab/WiSe22-M-PNLR-PR-TbNLP/blob/master/transformers.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing necessary packages (i.e. if on Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<snip>"
     ]
    }
   ],
   "source": [
    "# Huggingface evaluate for evaluation metrics\n",
    "# torchmetrics for no-hassle confusion matrices\n",
    "\n",
    "%pip install --upgrade datasets tokenizers transformers evaluate torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premise\n",
    "\n",
    "This notebook will guide you through the process of finetuning a transformer model using the ðŸ¤— Transformers library.\n",
    "\n",
    "First, we need to select a task and suitable dataset. Here, we will use the [Textual Entailment](https://aclweb.org/aclwiki/Recognizing_Textual_Entailment) task as an example. A suitable dataset can be found in the [SuperGLUE repository](https://www.tensorflow.org/datasets/catalog/super_glue#super_gluerte). While this is a TensorFlow dataset, we are lucky as it is mirrored on the ðŸ¤— Dataset hub.\n",
    "\n",
    "Thus, we can load the Recognizing Textual Entailment subset of the SuperGLUE dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.10733318328857422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519aaf1d7c0f424cb557e7b8a50b6911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 550152\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['premise', 'hypothesis', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "datasets.utils.logging.set_verbosity(\"error\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "snli_dataset = load_dataset(\"snli\")\n",
    "print(snli_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the dataset is already split into train, development and test splits.\n",
    "Each row contains four, but we only need to focus the premise, hypothesis and the label.\n",
    "\n",
    "The textual entailment task requires us to recognize, given two text fragments, whether the meaning of one text is entailed (*can be inferred*) from the other text.\n",
    "\n",
    "In this example, we will use a BERT-family model. With BERT, we formulate the entailment task as a simple classification task by concatenating the premise and hypothesis and training our classifier on the first token (the `[CLS]` token) of the input string:\n",
    "\n",
    "```\n",
    "\"[CLS] This is the premise, i.e. a text that means something. [SEP] This is the hypothesis, i.e. what we may be able to infer [SEP]\"\n",
    "```\n",
    "\n",
    "But let's first take a look at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': ['A person on a horse jumps over a broken down airplane.', 'A person on a horse jumps over a broken down airplane.'], 'hypothesis': ['A person is training his horse for a competition.', 'A person is at a diner, ordering an omelette.'], 'label': [1, 2]}\n",
      "{'premise': ['Two women are embracing while holding to go packages.', 'Two women are embracing while holding to go packages.'], 'hypothesis': ['The sisters are hugging goodbye while holding to go packages after just eating lunch.', 'Two woman are holding packages.'], 'label': [1, 0]}\n",
      "{'premise': ['This church choir sings to the masses as they sing joyous songs from the book at a church.', 'This church choir sings to the masses as they sing joyous songs from the book at a church.'], 'hypothesis': ['The church has cracks in the ceiling.', 'The church is filled with song.'], 'label': [1, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(snli_dataset['train'][:2])\n",
    "print(snli_dataset['validation'][:2])\n",
    "print(snli_dataset['test'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see above, the `test` split contains **unlabeled** samples, be we can ignore that for now.\n",
    "\n",
    "Let's construct the sentences as we outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [1, 2], 'text': ['A person on a horse jumps over a broken down airplane. [SEP] A person is training his horse for a competition.', 'A person on a horse jumps over a broken down airplane. [SEP] A person is at a diner, ordering an omelette.']}\n"
     ]
    }
   ],
   "source": [
    "prepared_dataset = snli_dataset.filter(lambda sample: sample['label'] >= 0)\n",
    "prepared_dataset = prepared_dataset.map(\n",
    "    lambda sample: {'text': f\"{sample['premise']} [SEP] {sample['hypothesis']}\"},\n",
    "    remove_columns=['premise', 'hypothesis']\n",
    ")\n",
    "print(prepared_dataset['train'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pre-Trained Models\n",
    "\n",
    "Now we need to load a pre-trained BERT model. You should use a subclass of [AutoModel](https://huggingface.co/docs/transformers/main/en/autoclass_tutorial).\n",
    "\n",
    "#### Load and instantiate a model for the textual entailment task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained('distilbert-base-uncased')\n",
    "config.num_labels = 3\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could use the [Trainer](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) class for easy training. You can follow the tutorial from [the official documentation](https://huggingface.co/docs/transformers/quicktour#trainer-a-pytorch-optimized-training-loop).\n",
    "\n",
    "#### Write the training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01731133460998535,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb84283721645b193979c04f452fede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017070293426513672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 550,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ab5119e43d4df4b133d71b7d598f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018691301345825195,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 10,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7304f3ab5d41d0bba5bc40a72ea831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"])\n",
    "\n",
    "dataset = prepared_dataset.map(tokenize_dataset, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/mnt/ssd2/team/stoeckel/distilbert-snli/\",\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=5000,\n",
    "    log_level=\"error\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load as load_metric\n",
    "precision = load_metric(\"precision\")\n",
    "recall = load_metric(\"recall\")\n",
    "f1 = load_metric(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred, logits=True):\n",
    "    predictions, labels = eval_pred\n",
    "    if logits:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    return precision.compute(predictions=predictions, references=labels, average=\"weighted\") \\\n",
    "            | recall.compute(predictions=predictions, references=labels, average=\"weighted\") \\\n",
    "            | f1.compute(predictions=predictions, references=labels, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3688' max='616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [616/616 19:39:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_pre_loss': 1.0994470119476318, 'dev_pre_precision': 0.33251962492686143, 'dev_pre_recall': 0.3351960983539931, 'dev_pre_f1': 0.17729867787420958, 'dev_pre_runtime': 6.6451, 'dev_pre_samples_per_second': 1481.092, 'dev_pre_steps_per_second': 92.7}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='103008' max='103008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [103008/103008 1:14:44, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.577400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.432000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.394100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.369400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.314900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.310100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.310500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.286800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.244700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.245600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.240100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.244200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_post_loss': 0.3272220194339752, 'dev_post_precision': 0.8997085763358726, 'dev_post_recall': 0.8996138996138996, 'dev_post_f1': 0.8996580795507888, 'dev_post_runtime': 6.1061, 'dev_post_samples_per_second': 1611.818, 'dev_post_steps_per_second': 100.882, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=lambda: AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config),\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(trainer.evaluate(dataset['validation'], metric_key_prefix='dev_pre'))\n",
    "trainer.train()\n",
    "print(trainer.evaluate(dataset['validation'], metric_key_prefix='dev_post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Test\n",
      "Test_Loss       0.341835\n",
      "Test_Precision  0.895842\n",
      "Test_Recall     0.895765\n",
      "Test_F1         0.895799\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics = trainer.evaluate(dataset['test'], metric_key_prefix='test')\n",
    "print(pd.DataFrame.from_dict({metric.title(): value for metric, value in metrics.items()}, columns=[\"Test\"], orient='index')[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training only the Classifier Head\n",
    "\n",
    "We can also freeze the weights of the transformer model and instead only train the weights of the classifier head on top.\n",
    "\n",
    "The classifier head consists of an projection layer (`model.pre_classifier`) and the classifier layer it self (`model.classifier`).\n",
    "\n",
    "Freezing layers in Pytorch can be done by setting the `Tensor.requires_grad` field to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2460' max='616' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [616/616 3:34:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_pre_loss': 1.0994470119476318, 'dev_pre_precision': 0.33251962492686143, 'dev_pre_recall': 0.3351960983539931, 'dev_pre_f1': 0.17729867787420958, 'dev_pre_runtime': 4.9685, 'dev_pre_samples_per_second': 1980.875, 'dev_pre_steps_per_second': 123.981}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='103008' max='103008' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [103008/103008 18:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.015800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.999200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.992800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.986700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.982600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.978700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.973000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.971500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.967400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.969900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.965300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.964300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.960600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.961200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.963700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.959300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>0.959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>0.958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>0.960400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dev_post_loss': 0.9226549863815308, 'dev_post_precision': 0.5693611488832085, 'dev_post_recall': 0.5689900426742532, 'dev_post_f1': 0.5683723041672778, 'dev_post_runtime': 6.1349, 'dev_post_samples_per_second': 1604.267, 'dev_post_steps_per_second': 100.409, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "def only_classifier_init():\n",
    "    _model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
    "    \n",
    "    # Freeze all parameters\n",
    "    for p in _model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # Unfreeze only projection and classifier layers\n",
    "    _model.pre_classifier.requires_grad_()\n",
    "    _model.classifier.requires_grad_()\n",
    "    return _model\n",
    "    \n",
    "\n",
    "oc_trainer = Trainer(\n",
    "    model_init=only_classifier_init,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(oc_trainer.evaluate(dataset['validation'], metric_key_prefix='dev_pre'))\n",
    "oc_trainer.train()\n",
    "print(oc_trainer.evaluate(dataset['validation'], metric_key_prefix='dev_post'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Test\n",
      "Test_Loss       0.920187\n",
      "Test_Precision  0.571025\n",
      "Test_Recall     0.570033\n",
      "Test_F1         0.569503\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics = oc_trainer.evaluate(dataset['test'], metric_key_prefix='test')\n",
    "print(pd.DataFrame.from_dict({metric.title(): value for metric, value in metrics.items()}, columns=[\"Test\"], orient='index')[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Training\n",
    "While using the trainer class is very convenient, if you have to run custom procedures during training, a regular training loop can be more accessible.\n",
    "\n",
    "We can re-use code from the datasets notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018524646759033203,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 550,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e0adaba1a54a5f96df352b6925d758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/550 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [1, 2], 'text': ['A person on a horse jumps over a broken down airplane. [SEP] A person is training his horse for a competition.', 'A person on a horse jumps over a broken down airplane. [SEP] A person is at a diner, ordering an omelette.'], 'input_ids': [[101, 1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012, 102, 1037, 2711, 2003, 2731, 2010, 3586, 2005, 1037, 2971, 1012, 102], [101, 1037, 2711, 2006, 1037, 3586, 14523, 2058, 1037, 3714, 2091, 13297, 1012, 102, 1037, 2711, 2003, 2012, 1037, 15736, 1010, 13063, 2019, 18168, 12260, 4674, 1012, 102]]}\n"
     ]
    }
   ],
   "source": [
    "def tokenize_custom(batch: dict):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        add_special_tokens=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=False,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "pt_dataset = prepared_dataset.map(tokenize_custom, batched=True)\n",
    "print(pt_dataset['train'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate(batch: list[dict]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Custom collation method that extracts the 'input_ids' and 'label' fields from the input dictionary,\n",
    "    pads the input_ids to equal length and stacks the input_ids and labels into tensors.\n",
    "    \"\"\"\n",
    "    input_ids = [torch.tensor(sample['input_ids']) for sample in batch]\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    label = torch.tensor([sample['label'] for sample in batch]).long()\n",
    "    return input_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "\n",
    "def train(model, dataloader, postfix_loss=True, postfix_freq=100):\n",
    "    \"\"\"\n",
    "    Run a training epoch for the model on the given dataloader. Each batch is moved to the models device,\n",
    "    so make sure the model is already on the correct device when you pass it to this method.\n",
    "    \n",
    "    If postfix_loss is True, the dataloader is expected to be a tqdm wrapper around the actual iterable.\n",
    "    The average loss of the `postfix_freq` last batches is then post-fixed to the tqdm progress bar.\n",
    "    \n",
    "    Returns the finetuned model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    criterion = CrossEntropyLoss()\n",
    "    loss_acc = []\n",
    "    for idx, (x, y) in enumerate(dataloader):\n",
    "        x_hat = model.forward(x.to(model.device))\n",
    "        \n",
    "        # Reset gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(x_hat.logits, y.long().to(model.device))\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        if postfix_loss:\n",
    "            loss_acc.append(float(loss.item()))\n",
    "            if idx % postfix_freq == 0:\n",
    "                loss = sum(loss_acc) / len(loss_acc)\n",
    "                dataloader.set_postfix_str(f\"loss: {loss:0.3f}\")\n",
    "                loss_acc = []\n",
    "    return model\n",
    "\n",
    "def predict(model, dataloader) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Run prediction for the data in the given dataloader.\n",
    "    \n",
    "    Returns the predictions and targets as integer tensors.\n",
    "    \"\"\"\n",
    "    preds = []\n",
    "    targets = []\n",
    "    model.eval()\n",
    "    for (x, y) in dataloader:\n",
    "        x_hat = model.forward(x.to(model.device))\n",
    "        pred = x_hat.logits.argmax(1)\n",
    "\n",
    "        preds.extend(pred.detach().cpu().tolist())\n",
    "        targets.extend(y.detach().cpu().tolist())\n",
    "\n",
    "    preds = torch.tensor(preds)\n",
    "    targets = torch.tensor(targets)\n",
    "    return preds, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01978588104248047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "374322459e0543a4a5a3bc48710d5648",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018480300903320312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 34336,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.036634206771850586,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 154,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Epoch 0 \n",
      "Precision  0.881906\n",
      "Recall     0.881731\n",
      "F1         0.881722\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01787567138671875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 34336,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.020497798919677734,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 154,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Epoch 1 \n",
      "Precision  0.891297\n",
      "Recall     0.890876\n",
      "F1         0.891040\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017020225524902344,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 34336,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016516923904418945,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 154,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Epoch 2 \n",
      "Precision  0.893199\n",
      "Recall     0.892400\n",
      "F1         0.892654\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm, trange\n",
    "from torch.optim import AdamW\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "# Use this lambda expression to filter out any parameters that do not .require_grad\n",
    "# and thus should not be changed (i.e. if we froze them on purpose)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)\n",
    "\n",
    "# Create one validation dataloader, no shuffeling necessary\n",
    "dev_dataloader = DataLoader(pt_dataset['validation'], batch_size=64, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "num_epochs = 3\n",
    "model.to(device)\n",
    "for epoch in trange(num_epochs, position=0):\n",
    "    # The training dataloader is re-initialized each epoch\n",
    "    # resulting in a different random sequence due to shuffle=True\n",
    "    # Remember that the batch_size affects the training!\n",
    "    train_dataloader = DataLoader(pt_dataset['train'], batch_size=16, shuffle=True, collate_fn=custom_collate)\n",
    "    model = train(model, tqdm(train_dataloader, position=1, leave=False))\n",
    "\n",
    "    preds, trues = predict(model, tqdm(dev_dataloader, position=1, leave=False))\n",
    "    metrics = compute_metrics((preds, trues), logits=False)\n",
    "    print(pd.DataFrame.from_dict({metric.title(): value for metric, value in metrics.items()}, columns=[f\"Epoch {epoch+1:d}\"], orient='index'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.010847091674804688,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 56,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 154,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Test\n",
      "Precision  0.889144\n",
      "Recall     0.888742\n",
      "F1         0.888862\n",
      "\n",
      "               Entailment  Unrelated  Contradiction\n",
      "Entailment           2988        298             82\n",
      "Unrelated             234       2774            211\n",
      "Contradiction          60        208           2969\n"
     ]
    }
   ],
   "source": [
    "from torchmetrics import ConfusionMatrix\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "test_dataloader = DataLoader(pt_dataset['test'], batch_size=64, shuffle=False, collate_fn=custom_collate)\n",
    "\n",
    "preds, trues = predict(model, tqdm(test_dataloader, position=1, leave=False))\n",
    "metrics = compute_metrics((preds, trues), logits=False)\n",
    "\n",
    "print(pd.DataFrame.from_dict({metric.title(): value for metric, value in metrics.items()}, columns=[\"Test\"], orient='index'))\n",
    "print()\n",
    "\n",
    "cm = ConfusionMatrix(num_classes=3)(preds, trues).numpy()\n",
    "labels = [\"Entailment\", \"Unrelated\", \"Contradiction\"]\n",
    "df = pd.DataFrame(cm, columns=labels, index=labels)\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Comparison\n",
    "#### Huggingface Trainer \n",
    "- smaller boilerplate ðŸ‘\n",
    "    - slightly larger overhead (`+11` min training time) ðŸ‘Ž\n",
    "- acceleration trivial ðŸ‘\n",
    "- very \"opaque\" ðŸ‘Ž\n",
    " \n",
    "#### Custom Training\n",
    "- larger boilerplate ðŸ‘Ž\n",
    "    - slightly less overhead (`-11` min training time) ðŸ‘\n",
    "- acceleration non-trivial ðŸ‘Ž\n",
    "- fully transparent ðŸ‘\n",
    "\n",
    "### Takeaway\n",
    "#### For Trainer\n",
    "- Prefer for pre-defined tasks\n",
    "\n",
    "#### For Custom\n",
    "- Prefer for custom tasks\n",
    "- Reduce boilerplate with projects from the PyTorch Ecosystem:\n",
    "    - [PyTorch-Lightning](https://pytorch-lightning.readthedocs.io/en/stable/)\n",
    "    - [Ignite](https://pytorch-ignite.ai/)\n",
    "- Accelerate training with projects from the PyTorch Ecosystem:\n",
    "    - [accelerate](https://huggingface.co/docs/accelerate/index)\n",
    "    - [Ray](https://docs.ray.io/en/latest)\n",
    "    \n",
    "#### For Transformers + Classifier Head Models\n",
    "- Finetuning the whole model takes about **four times longer** than just finetuning the classification head\n",
    "    - `74` min for fine-tuning\n",
    "    - `18` min for classifier head only\n",
    "- Finetuning the whole model yields about **two times larger improvement** on the SNLI dataset\n",
    "    - compared with random `33.33%`:\n",
    "        - `+56.64%` for fine-tuning (`89.97%`)\n",
    "        - `+23.62%` for classifier head only (`56.95%`)\n",
    "- See [`Adapters`](https://adapterhub.ml/) for trade-off between the two extremes\n",
    "    - Paper: [AdapterHub: A Framework for Adapting Transformers (EMNLP 2020)](https://aclanthology.org/2020.emnlp-demos.7/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a5edab282632443219e051e4ade2d1d5bbc671c781051bf1437897cbdfea0f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
